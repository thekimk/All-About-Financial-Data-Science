{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **데이터분석 단계(Data Analysis Cycle)**\n",
    "\n",
    "[![Open in Colab](http://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/thekimk/All-About-Machine-Learning/blob/main/Lecture2-5_DataAnalysis_UnsupervisedReduction_KK.ipynb)\n",
    "\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle0.png' width='800'></center>\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle1.png' width='800'></center>\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle2.png' width='800'></center>\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle3.png' width='800'></center>\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle4.png' width='800'></center>\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle5.png' width='800'></center>\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle6.png' width='800'></center>\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle7.png' width='800'></center>\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle8.png' width='800'></center>\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle9.png' width='800'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **비지도학습(Unsupervised) 알고리즘:** 차원변환\n",
    "\n",
    "---\n",
    "\n",
    "- **데이터분석 과정:** `학습` + `추론/예측`\n",
    "\n",
    "<center><img src='Image/Advanced/DataSplit_Concept1.png' width='700'></center>\n",
    "<center><img src='Image/Advanced/DataSplit_Concept2.png' width='700'></center>\n",
    "\n",
    "---\n",
    "\n",
    "**\"`비지도학습(Unsupervised Learning)`은 `정답 레이블`이 없기 때문에, 주로 데이터를 `새롭게 표현`하여 원래 데이터보다 `쉽게 해석`하거나 `특성들을 추가적으로 파악`하는데 주로 사용\"**\n",
    "\n",
    "- **차원변환:** `비지도학습` 알고리즘 중 `다차원 특성파악`을 위해 사용되는 가장 `기본(Baseline) 알고리즘`\n",
    "\n",
    "> (비수학적) **\"일상 속 문제의 다양한 풀이법들의 우선순위를 파악하는 문제\"**\n",
    "> - 유사한 고객 정보를 가진 사람들의 `공통된 쇼핑 취향`을 파악하면서 `그룹(레이블)`으로 `추론`하는 문제가 `군집문제` \n",
    "> - 다양한 고객 정보들에서 `신규 쇼핑 취향`과 같은 추가적인 특성을 파악하기 위해 고객 정보들을 `차원변환`하여 문제를 다각도로 살펴보는 것\n",
    "\n",
    "> (수학적) **\"특정 출력(종속변수)/입력(독립변수)의 구분이나 `관계 추론도 없고` 학습을 위한 `목표값도 없이`, 주어진 데이터의 `추가적인 특성확인`을 위해 `다른 차원으로 변환(Reduction)` 하는 알고리즘\"**\n",
    "> - **군집문제:** 데이터에서 `유사한 값`들을 가진 `군집(Cluster)을 예측`하며 `레이블`을 할당\n",
    "> - **차원변환:** 데이터를 다각도로 살펴보기 위해 `다른(차원) 관점`으로 `추가적인 변수를 예측`하며 특성 확인\n",
    ">\n",
    "> <center><img src='Image/Advanced/Unsupervised_Clustering_Reduction.png' width='900'></center>\n",
    "\n",
    "---\n",
    "\n",
    "<center><img src='Image/Advanced/ML_Type_Application_Upgrade.png' width='700'></center>  \n",
    "<center><img src='Image/Advanced/ML_Type_Category.png' width='700'></center>\n",
    "\n",
    "---\n",
    "\n",
    "| Clustering Algorithms | Association Rule Learning Algorithms | Dimensionality Reduction Algorithms | Ensemble Algorithms | Deep Learning Algorithms |\n",
    "|------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| <img src='Image/Advanced/Clustering-Algorithms.png' width='150'> | <img src='Image/Advanced/Assoication-Rule-Learning-Algorithms.png' width='150'> | <img src='Image/Advanced/Dimensional-Reduction-Algorithms.png' width='150'> | <img src='Image/Advanced/Ensemble-Algorithms.png' width='150'> | <img src='Image/Advanced/Deep-Learning-Algorithms.png' width='150'> |\n",
    "| k-Means | Apriori algorithm | Principal Component Analysis (PCA) | Boosting | Deep Boltzmann Machine (DBM) |\n",
    "| k-Medians | Eclat algorithm | Principal Component Regression (PCR) | Bootstrapped Aggregation (Bagging) | Deep Belief Networks (DBN) |\n",
    "| Expectation Maximisation (EM) | - | Partial Least Squares Regression (PLSR) | AdaBoost | Convolutional Neural Network (CNN) |\n",
    "| Hierarchical Clustering | - | Sammon Mapping | Stacked Generalization (blending) | Stacked Auto-Encoders |\n",
    "| - | - | Multidimensional Scaling (MDS) | Gradient Boosting Machines (GBM) | - |\n",
    "| - | - | Projection Pursuit | Gradient Boosted Regression Trees (GBRT) | - |\n",
    "| - | - | Linear Discriminant Analysis (LDA) | Random Forest | - |\n",
    "| - | - | Mixture Discriminant Analysis (MDA) | - | - |\n",
    "| - | - | Quadratic Discriminant Analysis (QDA) | - | - |\n",
    "| - | - | Flexible Discriminant Analysis (FDA) | - | - |\n",
    "\n",
    "---\n",
    "\n",
    "- **차원축소:** 차원변환 알고리즘은 다양하며, `매우 많은 변수들`로 구성된 `다차원 데이터`를 의미 있는 `특성을 유지`하면서 차원을 축소하여 `저차원 데이터`으로 `재표현(주로 축소)`하는 것\n",
    "\n",
    "> - **Why?**\n",
    ">\n",
    "> **(비수학적)**\n",
    ">\n",
    "> **\"현실 데이터는 `고차원 공간`으로 구성되어 있고 우리는 인위적으로 `저차원 공간` 데이터를 수집하며, 유사관점에서 보유하고 있는 `빅데이터(저차원+고차원)`를 사람이 인식가능한 `스몰데이터`로 표현 가능할 것**\n",
    ">\n",
    "> **\"단순히 데이터를 `압축`하는 것이 목적이 아니라, 일부 `정보 손실`이 있지만 이를 최소화 하면서 `고차원 특성 잘 파악` 하여 `잠재적인 요소`를 `추출`하는 것이 목적\"**\n",
    ">\n",
    "> **(수학적)** \n",
    ">\n",
    "> **\"`차원이 증가(변수의 증가)`할수록 데이터 간의 `거리가 기하급수적으로 증가`하기 때문에 `데이터의 밀도가 희소(Sparse)한 구조`를 가지게 되어 이를 사용한 모델링의 `알고리즘 성능 하락`하는 `차원의 저주` 발생\"**\n",
    ">\n",
    "> <center><img src='Image/Advanced/Dimension_Issue.png' width='700'></center>\n",
    ">\n",
    "> **\"`차원이 증가(변수의 증가)`하면 특정 독립변수는 `다른 독립변수에 영향을 받는(설명되는)` 경우가 발생하며 이를 `다중공선성(Multicolinearity)`라고 함\"**\n",
    ">\n",
    "> - **How?**\n",
    ">\n",
    "> **(1) 변수 선택(Feature/Variable Selection):** `특정 변수`가 `다른 변수들로 생성`될 수 있는 경우, 특정 변수의 `종속성이 강하다`고 하고 간단히 `제거를 통해 중요 변수들만 구성`하는 차원 축소\n",
    ">> - **장점:** `남은 변수들`을 통해 `중요도`와 `해석`이 용이\n",
    ">> - **단점:** `변수들` 간의 `종속성/상관성`을 명확하게 고려하기 `어려움`\n",
    ">> - `Ridge`, `Lasso`, `VIF(Variance Inflation Factor)` 등\n",
    ">\n",
    "> **(2) 변수 추출(Feature/Variable Extraction):** 변수들 간의 `상관관계`를 고려하여 `새로운 중요 변수`를 생성하는 차원축소\n",
    ">> - **장점:** `변수들` 간의 `상관성`을 고려하기 용이하고 `변수들`의 갯수를 `많이 줄일 수` 있음\n",
    ">> - **단점:** `새롭게 추출된` 변수들의 `의미나 해석`이 `어려움`\n",
    ">> - `PCA(Principal Component Analysis)`, `FA(Factor Analysis)` 등\n",
    "\n",
    "---\n",
    "\n",
    "- **종류:** 차원변환 알고리즘은 다양하고, 데이터 `특성/구조/목적`에 맞는 `적절한 선택` 필요\n",
    "\n",
    "> - **(Linear) Projection:** `선형`기준으로 데이터를 근사하여 `직관적`으로 차원을 축소\n",
    ">\n",
    "> - **(Non-linear) Manifold Learning:** 직관적으로 파악이 어려운 데이터의 `비선형적 관계`를 반영하여 차원을 축소\n",
    ">\n",
    "> | **접근방법** \t| **알고리즘** \t|\n",
    "|:---:\t|:---\t|\n",
    "| **(Linear) Projection** \t| Eigen Value Decomposition \t|\n",
    "|  \t| Singular Value Decomposition \t|\n",
    "|  \t| Truncated SVD \t|\n",
    "|  \t| Principal Component Analysis \t|\n",
    "|  \t| Factor Analysis \t|\n",
    "|  \t| Linear Discriminant Analysis \t|\n",
    "|  \t| Quadratic Discriminant Analysis \t|\n",
    "| **(Non-linear) Manifold Learning** \t| Kernel Principal Component Analysis \t|\n",
    "|  \t| Locally Linear Embedding (LLE) \t|\n",
    "|  \t| Isomap \t|\n",
    "|  \t| Multi Dimensional Scaling \t|\n",
    "|  \t| Spectral Embedding \t|\n",
    "|  \t| t-distributed Stochastic Neighbor Embedding (t-SNE) \t|\n",
    "|  \t| Autoencoders \t|\n",
    "|  \t| Self Organizaing Map (SOP) \t|\n",
    "\n",
    "---\n",
    "\n",
    "- **Target Algorithm:**\n",
    "> - `Eigen Value Decomposition`\n",
    "> - `Singular Value Decomposition`\n",
    "> - `Truncated SVD`\n",
    "> - `Principal Component Analysis`\n",
    "> - Principal Component Regression\n",
    "> - Factor Analysis\n",
    "> - Linear Discriminant Analysis\n",
    "> - Quadratic Discriminant Analysis\n",
    "> - Mixture Discriminant Analysis\n",
    "> - Non-Negative Matrix Factorization\n",
    "> - Locally Linear Embedding\n",
    "> - t-distributed Stochastic Neighbor Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **예제 데이터셋(Dataset)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## statsmodels 모듈 사용 데이터셋\n",
    "\n",
    "```python\n",
    "# 라이브러리 불러오기\n",
    "import statsmodels.api as sm\n",
    "```\n",
    "\n",
    "> - 대기중 `CO2농도` 데이터:\n",
    "> ```python\n",
    "> data = sm.datasets.get_rdataset(\"CO2\", package=\"datasets\")\n",
    "> ```\n",
    "> - 황체형성 `호르몬(Luteinizing Hormone)`의 수치 데이터:\n",
    "> ```python\n",
    "> data = sm.datasets.get_rdataset(\"lh\")\n",
    "> ```\n",
    "> - 1974~1979년 사이의 영국의 `호흡기 질환 사망자 수` 데이터:\n",
    "> ```python\n",
    "> data = sm.datasets.get_rdataset(\"deaths\", \"MASS\")\n",
    "> ```\n",
    "> - 1949~1960년 사이의 `국제 항공 운송인원` 데이터:\n",
    "> ```\n",
    "> data = sm.datasets.get_rdataset(\"AirPassengers\")\n",
    "> ```\n",
    "> - 미국의 `강수량` 데이터:\n",
    "> ```python\n",
    "> data = sm.datasets.get_rdataset(\"precip\")\n",
    "> ```\n",
    "> - `타이타닉호의 탑승자`들에 대한 데이터:\n",
    "> ```python\n",
    "> data = sm.datasets.get_rdataset(\"Titanic\", package=\"datasets\")\n",
    "> ```\n",
    "\n",
    "- **data가 포함하는 정보:**\n",
    "> - `package`: 데이터를 제공하는 R 패키지 이름\n",
    "> - `title`: 데이터 이름\n",
    "> - `data`: 데이터를 담고 있는 데이터프레임\n",
    "> - `__doc__`: 데이터에 대한 설명 문자열(R 패키지의 내용 기준)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn 모듈 사용 데이터셋\n",
    "\n",
    "**1) 패키지에 포함된 데이터(`load 명령어`)**\n",
    "    \n",
    "```python\n",
    "# 라이브러리 불러오기\n",
    "from sklearn.datasets import load_boston\n",
    "```\n",
    "\n",
    "> - load_boston: 회귀용 `보스턴 집값`\n",
    "> ```python\n",
    "> raw = load_boston()\n",
    "> print(raw.DESCR)\n",
    "> print(raw.keys())\n",
    "> print(raw.data.shape, raw.target.shape)\n",
    "> ```\n",
    "> - load_diabetes: 회귀용 `당뇨병` 자료\n",
    "> - load_linnerud: 회귀용 `linnerud` 자료\n",
    "> - load_iris: 분류용 `붓꽃(iris)` 자료\n",
    "> - load_digits: 분류용 `숫자(digit) 필기 이미지` 자료\n",
    "> - load_wine: 분류용 `포도주(wine) 등급` 자료\n",
    "> - load_breast_cancer: 분류용 `유방암(breast cancer)` 진단 자료\n",
    "\n",
    "**2) 인터넷에서 다운로드할 수 있는 데이터(`fetch 명령어`)**\n",
    "\n",
    "```python\n",
    "# 라이브러리 불러오기\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "```\n",
    "\n",
    "> - fetch_california_housing: : 회귀용 `캘리포니아 집값`\n",
    "> ```python\n",
    "> raw = fetch_california_housing()\n",
    "> print(raw.DESCR)\n",
    "> print(raw.keys())\n",
    "> print(raw.data.shape, raw.target.shape)\n",
    "> ```\n",
    "> - fetch_covtype : 회귀용 `토지` 조사 자료\n",
    "> - fetch_20newsgroups : `뉴스 그룹` 텍스트 자료\n",
    "> - fetch_olivetti_faces : `얼굴 이미지` 자료\n",
    "> - fetch_lfw_people : `유명인 얼굴 이미지` 자료\n",
    "> - fetch_lfw_pairs : `유명인 얼굴 이미지` 자료\n",
    "> - fetch_rcv1 : 로이터 `뉴스 말뭉치`\n",
    "> - fetch_kddcup99 : `Kddcup 99 Tcp dump` 자료\n",
    "\n",
    "**3) 확률분포를 사용한 가상 데이터(`make 명령어`)**\n",
    "\n",
    "```python\n",
    "# 라이브러리 불러오기\n",
    "from sklearn.datasets import make_regression\n",
    "```\n",
    "\n",
    "> - make_regression: `회귀용` 가상 데이터\n",
    "> ```python\n",
    "> X, y, c = make_regression(n_samples=100, n_features=10, n_targets=1, bias=0, noise=0, coef=True, random_state=0)\n",
    "> ```\n",
    "> - make_classification: `분류용` 가상 데이터 생성\n",
    "> - make_blobs: `클러스터링용` 가상 데이터 생성\n",
    "\n",
    "---\n",
    "\n",
    "**4) `load/fetch 명령어 데이터`에서 raw가 포함하는 정보:** Bunch 라는 `클래스 객체 형식`으로 생성\n",
    "\n",
    "> - `data`: (필수) 독립 변수 ndarray 배열\n",
    "> - `target`: (필수) 종속 변수 ndarray 배열\n",
    "> - `feature_names`: (옵션) 독립 변수 이름 리스트\n",
    "> - `target_names`: (옵션) 종속 변수 이름 리스트\n",
    "> - `DESCR`: (옵션) 자료에 대한 설명\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **전처리 방향(Preprocessing)**\n",
    "\n",
    "- **목표:** \n",
    "> - 대량으로 수집된 데이터는 `그대로 활용 어려움`\n",
    "> - `잘못 수집/처리 된 데이터`는 엉뚱한 결과를 발생\n",
    "> - 알고리즘이 `학습이 가능한 형태`로 데이터를 정리\n",
    "<center><img src='Image/Advanced/DataAnalysis_Time.jpg' width='500'></center> \n",
    "---\n",
    "\n",
    "> **일반적인 전처리 필요항목:**  \n",
    "> - 데이터 결합\n",
    "> - 결측값 처리\n",
    "> - 이상치 처리\n",
    "> - 자료형 변환\n",
    "> - 데이터 분리\n",
    "> - 데이터 변환\n",
    "> - `스케일 조정`: `단위가 큰 변수`들에 의해 `차원변환 결과가 왜곡` 될 수 있음!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **함수세팅 및 추정 방향(Modeling):** Principal Component Analysis\n",
    "\n",
    "> - **`Eigen Value Decomposition`**\n",
    "> - **`Singular Value Decomposition`**\n",
    "> - **`Truncated SVD`**\n",
    "> - **`Principal Component Analysis`**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 차원변환 필수 개념\n",
    "\n",
    "> **\"`차원변환` 알고리즘들은 대부분 `차원을 다루는` 선형대수를 활용하여 `벡터기반`으로 `개발`되고 만들어진 알고리즘\"**\n",
    "\n",
    "**0) 차원(Dimension):** `공간` 내에서 `데이터의 위치`를 나타내기 위해 필요한 `축의 갯수`\n",
    "\n",
    "<center><img src='Image/Advanced/Dimension_Levels.png' width='500'>(https://commons.wikimedia.org/wiki/File:Dimension_levels.svg)</center>\n",
    "\n",
    "- **`변수 = 차원`:** 데이터가 `n`개의 변수를 가지면, `n`차원의 좌표축 상에 `모든 데이터 위치` 표현 가능\n",
    "\n",
    "> - 각 `변수의 값들`을 사용해 `공간상`에 하나의 방향을 가진 `직선(벡터)로 표현`하기 때문에 각 변수를 `하나의 공간표현 차원`으로 인식\n",
    ">\n",
    "> <center><img src='Image/Advanced/Dimension_withTable.png' width='800'></center>\n",
    ">\n",
    "> - 우리가 `2차원 데이터`, `3차원 데이터` 등으로 부르는 경우, 변수의 수가 아니라 `인덱스, 변수, 시간 등` 사람의 `눈으로 해석`을 위해 필요한 `축의 갯수`로 의미 다름\n",
    ">\n",
    "> <center><img src='Image/Advanced/Dimension_withInterpretation.png' width='300'></center>\n",
    "\n",
    "---\n",
    "\n",
    "**1) 기저(Basis):** `데이터`의 `독립 변수 갯수`를 파악하기 위해 `선형대수의 벡터` 기반의 용어\n",
    "\n",
    "[![Linear transformations and matrices](https://img.youtube.com/vi/kYB8IZa5AuE/0.jpg)](https://youtu.be/kYB8IZa5AuE)\n",
    "\n",
    "|  | **데이터1** |  |  |  | **데이터2** |  |\n",
    "|---|:---:|:---:|:---:|---|:---:|:---:|\n",
    "|  | **변수1** | **변수2** |  |  | **변수1** | **변수2** |\n",
    "| **샘플1** | 2 | 0 |  | **샘플1** | 1 | 2 |\n",
    "| **샘플2** | 0 | 1 |  | **샘플2** | 0 | 0 |\n",
    "\n",
    "<center><img src='Image/Advanced/Reduction_Basis.png' width='800'></center>\n",
    "\n",
    "> - `데이터`의 `독립 차원/변수 갯수` 파악하기 위해 `선형대수에선 Basis 갯수` 추정\n",
    "> - `Basis`의 갯수만큼의 `데이터 표현 공간` 확인 가능\n",
    ">> - `데이터1`은 `최대 2차원` 공간의 `데이터 표현 가능: (5,3) 가능`\n",
    ">> - `데이터2`는 `최대 1차원` 공간의 `데이터 표현 가능: (5,3) 불가능`\n",
    "\n",
    "---\n",
    "\n",
    "- **`컴퓨터가 인식`하는 데이터는 최대 `독립변수의 수`만큼:**\n",
    "\n",
    "> - `데이터1`은 변수1 & 변수2가 `다른 데이터`로 인식\n",
    "> - `데이터2`는 변수1 & 변수2를 `같은 데이터`로 인식\n",
    "> - `회귀문제/분류문제`에서 `독립변수의 수`가 중요했던 이유\n",
    ">\n",
    "> $$W = (X^TX)^{-1}X^TY$$\n",
    ">\n",
    "> **모든 $X$가 `독립은 아님`**\n",
    ">\n",
    "> = (`기저의 갯수` $\\ne$ `입력변수의 갯수`)\n",
    ">\n",
    "> = `역행렬이 미존재`\n",
    ">\n",
    "> = $X$가 Full Rank가 아님  \n",
    ">\n",
    "> = $X^TX$가 양의 정부호(Positive Definite)가 아님\n",
    ">\n",
    "> => **`Full Rank / Positive Definite` 계산을 통해 쉽게 `Basis 갯수` & `독립변수 갯수` & `역행렬 존재` 확인 가능**\n",
    ">> `Full Rank`     \n",
    ">>\n",
    ">> = (`기저의 갯수` = `입력변수의 갯수`)    \n",
    ">>\n",
    ">> = `역행렬 존재`\n",
    ">>\n",
    ">> = 모든 $X$가 `독립`\n",
    "\n",
    "- **예시:** 주소를 말하시오!\n",
    "\n",
    "|  | **예시1** | **예시2** |\n",
    "|---|:---:|:---:|\n",
    "| **주소** | 서울시 용산구 무슨동 | 서울시 청주시 김해시 |\n",
    "| **이해도** | 어디에 사는지 바로 이해가능 | 어디에 사는지 이해불가능 |\n",
    "| **입력변수 수** | 3개(서울시/용산구/무슨동) | 3개(서울시/청주시/김해시) |\n",
    "| **독립변수 수** | 3개(시/구/동) | 1개(시) |\n",
    "| **Basis 수** | 3개(시/구/동) | 1개(시) |\n",
    "| **역행렬 존재** | 존재 <BR> => `3차원`에 사는데 `3차원`으로 말하니 `이해가능` | 미존재 <br> => `3차원`에 사는데 `1차원`으로 말하면 `이해못함` |\n",
    "\n",
    "---\n",
    "\n",
    "**2) 고유벡터(Eigenvector) & 고유값(Eigenvalue):**\n",
    "\n",
    "<center><img src='Image/Advanced/Reduction_EigenComparison.png' width='500'></center>\n",
    "\n",
    "> (1) `원데이터`에 어떠한 `차원변환`을 하더라도\n",
    ">\n",
    "> (2) `차원 변환된 데이터`가 `서로 독립`이고\n",
    ">\n",
    "> (3) `차원 변환된 데이터`의 각 변수의 `값이 모두 0이 아니고`\n",
    ">\n",
    "> (4) `원데이터의 변수`와 `차원 변환된 데이터 변수`가 크기는 달라도 `방향만 같으면`\n",
    ">\n",
    "> (5-1) **`Eigenvector`:** `차원 변환된 데이터 변수`\n",
    ">> - 차원 변환시, 변화되는 `방향`\n",
    ">\n",
    "> (5-2) **`Eigenvalue`:** `원데이터의 변수` 대비 `차원 변환된 데이터 변수`의 `크기 변화`\n",
    ">> - 차원 변환시, 변화되는 `크기`\n",
    "\n",
    "<center><img src='Image/Advanced/Reduction_Eigen.png' width='500'></center>\n",
    "\n",
    "- **예시:**\n",
    "\n",
    "|  | **데이터1** |  |  |  | **데이터2** |  |\n",
    "|---|:---:|:---:|:---:|---|:---:|:---:|\n",
    "|  | **변수1** | **변수2** |  |  | **변수1** | **변수2** |\n",
    "| **샘플1** | 2 | 0 |  | **샘플1** | 1 | 2 |\n",
    "| **샘플2** | 0 | 1 |  | **샘플2** | 0 | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T06:27:47.138946Z",
     "start_time": "2022-04-25T06:27:46.971945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data1: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "matrix([[2, 0],\n",
       "        [0, 1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Data2: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "matrix([[1, 2],\n",
       "        [0, 0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Eigenvalue & Eigenvector of Data1: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([2., 1.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Eigenvalue & Eigenvector of Data2: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([1., 0.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.89442719],\n",
       "       [ 0.        ,  0.4472136 ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 고유값 및 고유벡터 추정\n",
    "import numpy as np\n",
    "from numpy.linalg import eig\n",
    "\n",
    "data1 = [[2,0],[0,1]]\n",
    "data2 = [[1,2],[0,0]]\n",
    "display('Data1: ', np.matrix(data1))\n",
    "display('Data2: ', np.matrix(data2))\n",
    "display('Eigenvalue & Eigenvector of Data1: ', eig(data1)[0], eig(data1)[1])\n",
    "display('Eigenvalue & Eigenvector of Data2: ', eig(data2)[0], eig(data2)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **데이터1 풀이:** `Full Rank`인 데이터1은 `고유값`과 `고유벡터`가 아래와 같고 총 `2개`\n",
    "<center><img src='Image/Advanced/Reduction_Eigen1.png' width='900'></center>\n",
    "\n",
    "- **데이터2 풀이:** `Full Rank`가 `아닌` 데이터2은 `고유값`과 `고유벡터`가 아래와 같고 총 `1개`\n",
    "<center><img src='Image/Advanced/Reduction_Eigen2.png' width='900'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 차원변환 해결을 위한 세팅 및 추정\n",
    "\n",
    "> - **`Eigen Value Decomposition`**\n",
    "> - **`Singular Value Decomposition`**\n",
    "> - **`Truncated SVD`**\n",
    "> - **`Principal Component Analysis`**\n",
    "\n",
    "---\n",
    "\n",
    "**1) 고유값 분해(EigenValue Decomposition, EVD):** \n",
    "\n",
    "<center><img src='Image/Advanced/Reduction_Eigen.png' width='500'></center>\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{차원변환 유도행렬 } A &= V \\Sigma V^{-1}. \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "> **\"데이터를 차원변환 시키는 행렬이 `정방행렬(정사각형 행렬)`이고 `모든 차원변수가 독립(Full Rank)`인 경우, `3개 행렬`로 분해 가능\"**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A = \\begin{bmatrix}1.2 & -0.5 \\\\ -1.5 & 1.7\\end{bmatrix}, \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Eigenvalues: } \\lambda_1 = 0.5486 \\text{ and } \\lambda_2 =2.3514, \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Eigenvectors: } \\vec{v}_1 = \\begin{bmatrix}0.6089 \\\\ 0.7933\\end{bmatrix},\n",
    "\\vec{v}_2 = \\begin{bmatrix}-0.3983 \\\\ 0.9172\\end{bmatrix}. \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V =  \\begin{bmatrix} \\vec{v}_1, \\vec{v}_2\\end{bmatrix} = \\begin{bmatrix}0.6089 & -0.3983 \\\\ 0.7933 & 0.9172\\end{bmatrix}, \\\\\n",
    "\\Sigma = \\begin{bmatrix}\\lambda_1 & 0 \\\\ 0 & \\lambda_2\\end{bmatrix} = \\begin{bmatrix}0.5486 & 0 \\\\ 0 & 2.3514\\end{bmatrix}, \\\\\n",
    "V^{-1} = \\text{Inverse of }V = \\begin{bmatrix}1.0489 & 0.4555 \\\\ -0.9072 & 0.6963\\end{bmatrix}. \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{A} &= V \\Sigma V^{-1} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Rightarrow \\begin{bmatrix}1.2 & -0.5 \\\\ -1.5 & 1.7\\end{bmatrix}=\\begin{bmatrix}0.6089 & -0.3983 \\\\ 0.7933 & 0.9172\\end{bmatrix} \\begin{bmatrix}0.5486 & 0 \\\\ 0 & 2.3514\\end{bmatrix}\\begin{bmatrix}1.0489 & 0.4555 \\\\ -0.9072 & 0.6963\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<center><img src='Image/Advanced/Reduction_Eigen.png' width='500'></center>\n",
    "\n",
    "> - **`Eigenvector`:** 차원 변환시, 변화되는 `방향`\n",
    "> - **`Eigenvalue`:** 차원 변환시, 변화되는 `크기`\n",
    ">\n",
    "> $\\Rightarrow$ **\"`원데이터`를 `특정방향`으로 바꾸고 `크기를 확대/축소` 한 후 `원래방향`으로 `복귀`\"**\n",
    "\n",
    "<center><img src='Image/Advanced/Singular-Value-Decomposition.png' width='500'>(https://commons.wikimedia.org/wiki/File:Singular-Value-Decomposition.svg)</center>\n",
    "\n",
    "---\n",
    "\n",
    "**2) 특이값 분해(Singular Value Decomposition, SVD):** \n",
    "\n",
    "> **\"데이터를 차원변환 시키는 행렬이 또는 Raw 데이터가, `독립이 아니더라도(Not Full Rank)` 그리고 `어떠한 형태(정사각형 직사각형 포함)`라도 `3개의 행렬`로 분해 가능\"**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A &= U \\Sigma V^{T} \\\\\n",
    "& \\text{where } A \\text{ is } m \\times n \\text{ matrix } (m \\times n), \\\\\n",
    "& U \\text{ is } m \\times m \\text{ eigenvector matrix of } A \\times A^{T}, \\\\\n",
    "& V \\text{ is } n \\times n \\text{ eigenvector matrix of } A^{T} \\times A, \\\\\n",
    "& \\text{and } \\Sigma \\text{ is } m \\times n \\text{ diagonal matrix with eigenvalues } \\sqrt{\\lambda_{i}}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src='Image/Advanced/SingularValueDecomposition_Structure.png' width='600'>(https://www.pikpng.com/transpng/hxRRmbR/)</center>\n",
    "\n",
    "> - `EVD`와 해석 `의미는 비슷`\n",
    "> - `SVD`는 직사각형을 포함한 `모든 행렬`에서 적용 가능하다는 `차이`\n",
    "> - `EVD`에서 $A$의 `고유벡터 행렬`이었던 $V$와 $V^{-1}$가 `SVD`에서는 각각 $AA^{T}$와 $A^{T}A$의 `고유벡터 행렬`\n",
    "> - 제곱으로 계산된 `고유벡터` 대응 `고유값`은 `길이`가 `2번` 곱해진 것이기 때문에, `루트`를 사용하여 `원래길이로 복원`\n",
    "\n",
    "---\n",
    "\n",
    "- **4가지 종류:** `Full SVD` 사용 경우는 드물고, `Reduced SVD` 사용 일반적\n",
    "\n",
    "> **(1) Full SVD:**\n",
    ">\n",
    "> <center><img src='Image/Advanced/SVD_Full.png' width='600'></center>\n",
    ">\n",
    "> **(2) Thin SVD:** $\\Sigma$에서 `비대각파트`에 `0`으로 구성된 부분을 `없애고` $U$에서는 이에 `대응되는 열벡터 제거`한 형태 (`A 변경 없음`)\n",
    ">\n",
    "> <center><img src='Image/Advanced/SVD_Thin.png' width='600'></center>\n",
    ">\n",
    "> **(3) Compact SVD:** $\\Sigma$에서 `비대각파트` 뿐만 아니라 `대각파트`에도 `0`으로 구성된 `고유값` 대응 부분도 제거한 형태 (`A 변경 없음`)\n",
    ">\n",
    "> <center><img src='Image/Advanced/SVD_Compact.png' width='600'></center>\n",
    ">\n",
    "> **(4) Truncated SVD:** $\\Sigma$에서 `비대각파트` , `0인 고유값 대각파트` 뿐만 아니라 `0이 아닌 고유값 대각파트`도 제거한 형태 (`A 변경되어 원래의 A 복원 불가!`)\n",
    ">\n",
    "> <center><img src='Image/Advanced/SVD_Truncated.png' width='500'>(https://darkpgmr.tistory.com/106)</center>\n",
    "> \n",
    "> **\"연산이 많을 수 밖에 없은 `빅데이터`의 차원변환시 `Reduced SVD`를 통해 `연산량 절감` 가능\"**\n",
    ">\n",
    "> **\"`Compact SVD`까지는 연산량을 줄여도 `차원변환 정도`가 그대로지만, `Truncated SVD`에서는 `차원변환 정도가 줄어들기 시작`\"**\n",
    ">\n",
    "> **\"그럼에도 `Truncated SVD`를 사용하는 큰 이유는, `연산량`이 `상당히 감소`함에도 불구하고 `차원변환 정도`는 큰 차이가 없기 때문\"**\n",
    ">\n",
    "> <center><img src='Image/Advanced/SVD_Truncated_Example.png' width='900'></center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) 주성분 분석(Principal Component Analysis, PCA):** 최적의 `차원/변수 조합`을 찾아 `변수선택 및 차원축소` 방법론 중 하나\n",
    "\n",
    "- **원리:** 모든 차원의 특성을 `최대로 유지`하면서 원데이터의 `손실을 최소화` 하기 위해 `높은 분산을 갖는 기저(Basis)`를 찾아 `차원축소`\n",
    "\n",
    "> **\"비수학적(기하학적)\":** 데이터의 특성을 잘 표현하는 `데이터의 차원(주성분)`을 뽑아 `데이터의 양`을 줄이는 것\n",
    ">\n",
    "> (1) 데이터의 특징을 잘 표현(분산이 큰)하는 `주축(Principle)`을 찾음\n",
    ">\n",
    "> (2) 주축으로  데이터를 `차원변환`한 상태에서 분산이 큰 `성분축(Componemt)`을 찾음\n",
    "> \n",
    "> (3) `성분축`을 기준으로 (2)를 지속 반복하여 `변수의 갯수`만큼의 모든 `주축+성분축` 찾음\n",
    ">\n",
    "> (4) `변수의 갯수`보다 적은 `주축+성분축`만 골라 최종 `차원변환 행렬` 생성\n",
    ">\n",
    "> (5) `최종 차원변환 행렬`에 데이터를 곱하여 `새로운 저차원 데이터` 생성\n",
    ">\n",
    "> <center><img src='Image/Advanced/PCA_2D_Projection1.png' width='500'></center>\n",
    ">\n",
    "> <center><img src='Image/Advanced/PCA_2D_Projection2.png' width='600'>(https://tyami.github.io/machine%20learning/PCA/)</center>\n",
    ">\n",
    "<!-- > <center><img src='Image/Advanced/PCA_2DFinal.png' width='800'>(https://subinium.github.io/MLwithPython-3-4/)</center> -->\n",
    ">\n",
    "> <center><img src='Image/Advanced/PCA_2D_Ex.png' width='800'>(https://www.mdpi.com/2076-3417/11/9/3780)</center>\n",
    "\n",
    "> **\"수학적\":** 변수들 간에 존재하는 `분산(상관관계)`를 이용하여 `주성분`을 추출하면서 차원을 축소하는 기법\n",
    ">\n",
    "> (1) 데이터의 `표준화/정규화`하여 `크기를 조정`하고 평균으로 `원점을 옮김`\n",
    ">\n",
    "> $$\n",
    "\\begin{aligned}\n",
    "X_{s} = \\frac{X - \\mu}{\\sigma}\n",
    "\\end{aligned}\n",
    "$$\n",
    ">\n",
    "> (2) 데이터의 `공분산 행렬` 계산\n",
    ">\n",
    "> - **공분산(Covariance):** `2개 이상`의 변수들의 `상관관계`\n",
    "> - 공분산은 `변수의 수`에 따라 `증가`하기 때문에, `변수의 수`로 나눔\n",
    ">\n",
    "> $$\n",
    "\\begin{aligned}\n",
    "Cov(X_{s}) = \\frac{1}{n - 1} X_{s}^{T} X_{s}\n",
    "\\end{aligned}\n",
    "$$\n",
    ">\n",
    "> (3) `공분산 행렬`에서 `SVD`를 사용하여 `고유벡터 및 고유값` 추정\n",
    ">\n",
    "<!-- > $$\n",
    "\\begin{aligned}\n",
    "X_{s} &= U \\Sigma V^{T} \\\\\n",
    "& \\text{where } A \\text{ is } m \\times n \\text{ matrix } (m \\times n), \\\\\n",
    "& U \\text{ is } m \\times m \\text{ eigenvector matrix of } X_{s} \\times X_{s}^{T}, \\\\\n",
    "& V \\text{ is } n \\times n \\text{ eigenvector matrix of } X_{s}^{T} \\times X_{s}, \\\\\n",
    "& \\text{and } \\Sigma \\text{ is } m \\times n \\text{ diagonal matrix with eigenvalues } \\sqrt{\\lambda_{i}}.\n",
    "\\end{aligned}\n",
    "$$ -->\n",
    ">\n",
    "> (4) `고유값`이 큰 순서대로 `정렬`\n",
    ">\n",
    "> - `고유값`이 가장 큰 경우의 `고유벡터`는 데이터에서 `가장 큰 분산을 가진 기저(Basis)`\n",
    "> - `고유값` = `고유벡터의 크기` = `데이터의 분산`\n",
    "> - 고유값의 크기만큼의 `변수의 분산 크기`\n",
    "> - `고유값의 수` = `고유벡터의 수` = `차원/변수의 수`\n",
    "> - 변수의 수만큼의 `고유값/고유벡터` \n",
    ">\n",
    "> $$\n",
    "\\begin{aligned}\n",
    "V =  \\begin{bmatrix} \\vec{v}_1, \\vec{v}_2, \\vec{v}_3, ...\\end{bmatrix} \n",
    "\\end{aligned}\n",
    "$$\n",
    "> - $\\vec{v}_1$이 가장 분산이 큰 고유벡터, $\\vec{v}_2$는 다음으로 분산이 큰 고유벡터\n",
    "> - $\\vec{v}_1, \\vec{v}_2, \\vec{v}_3, ...$는 `서로 독립`이며 공간상에서는 `직각`으로 표현됨\n",
    ">\n",
    "> (5) 지정된 `최소 분산 크기 이상`을 설명하도록, `k(변수의 수보다 작은)`번째 `고유벡터`까지 선택\n",
    ">\n",
    "> $$\n",
    "\\begin{aligned}\n",
    "Var(X_{s}) & \\simeq Var(X_{s,k}) \\\\\n",
    "& = Var(X_{s,1}) + Var(X_{s,2}) + ... + Var(X_{s,k}) \\\\\n",
    "& = \\lambda_{1} + \\lambda_{2} + ... + \\lambda_{k}\n",
    "\\end{aligned}\n",
    "$$\n",
    ">\n",
    "> - **PVE(Proportion of Variance Explained):** `주성분들`의 `분산 비율`\n",
    "> - 일반적으로 모든 주성분을 사용하지 않고, `처음 몇개`의 주성분만 사용\n",
    "> - `몇개의 주성분이 필요한지` `Scree Plot` 시각화하고 `총합은 100%`\n",
    "> - `적은 변수` 일수록 좋고 `많은 분산`을 커버할수록 좋음\n",
    "> - 누적 분산의 기울기가 급변하는 지점을 흔히 `Elbow(팔꿈치)`라고 함\n",
    ">\n",
    "> $$\n",
    "\\begin{aligned}\n",
    "\\frac{\\sum_{j=1}^{k} \\lambda_{j}}{\\sum_{j=1}^{n} \\lambda_{j}} = 90 \\% ?\n",
    "\\end{aligned}\n",
    "$$\n",
    ">\n",
    "> <center><img src='Image/Advanced/PCA_ScreePlot1.png' width='800'>(https://www.researchgate.net/figure/Principal-components-analysis-scree-plot_fig2_350281842)</center>\n",
    ">\n",
    "> <center><img src='Image/Advanced/PCA_ScreePlot2.png' width='800'>(https://bioconductor.org/packages/devel/bioc/vignettes/PCAtools/inst/doc/PCAtools.html)</center>\n",
    ">\n",
    "> (6) `원데이터`에 `선택된 고유벡터 행렬`로 차원변환을 하면 `새로운 저차원 데이터`\n",
    ">\n",
    "> <center><img src='Image/Advanced/PCA_Visualization.png' width='800'></center>\n",
    "\n",
    "---\n",
    "\n",
    "<center><img src='Image/Advanced/PCA_MultiEx.png' width='800'></center>\n",
    "<center><img src='Image/Advanced/PCA_MultiExPC1PC2.png' width='500'>(https://www.mdpi.com/2076-3417/11/9/3780)</center>\n",
    "\n",
    "---\n",
    "\n",
    "- **정리:** 모든 차원의 특성을 `최대로 유지`하면서 원데이터의 `손실을 최소화` 하기 위해 `높은 분산을 갖는 기저(Basis)`를 찾아 `차원축소`\n",
    "\n",
    "> - `데이터의 분포 특성`은 일반적으로 평균과 분산 등이 있는데, PCA는 데이터의 퍼진 정도인 `분산(상관관계)` 사용\n",
    "> - 차원축소시 `벡터와 행렬 연산`, 최대화 분산 파악을 위한 `고유값, 고유벡터, 공분산, 라그랑주승수 등` 여러가지 `선형대수 이론` 활용\n",
    "> - `원데이터`의 `분포 특성`을 유지한 채, `차원 축소`하는 기법으로 `특이값 분해(SVD)`방법론 사용\n",
    ">\n",
    "> **(1) Full SVD:**\n",
    ">\n",
    "> <center><img src='Image/Advanced/SVD_Full.png' width='600'></center>\n",
    ">\n",
    "> **(4) Truncated SVD:** $\\Sigma$에서 `비대각파트` , `0인 고유값 대각파트` 뿐만 아니라 `0이 아닌 고유값 대각파트`도 제거한 형태 (`A 변경되어 원래의 A 복원 불가!`)\n",
    ">\n",
    "> <center><img src='Image/Advanced/SVD_Truncated.png' width='500'>(https://darkpgmr.tistory.com/106)</center>\n",
    "\n",
    "- **장단점:**\n",
    "\n",
    "> **\"장점\"**\n",
    ">\n",
    "> - 데이터의 차원/변수의 수가 `크면` 데이터를 한눈에 `인식하기 어려운데`, PCA가 `도움`\n",
    "> - 데이터의 `주요한 특징을 추출`하기 때문에 `지도학습`을 위한 `전처리, 시각화`의 도구로도 많이 사용\n",
    "> - 많은 양의 정보를 `효과적`으로 `시각화` 가능\n",
    "> - 독립변수들이 독립적이지 않아 발생하는 `다중공선성(Multicollinearity)`를 해결하는데 기여\n",
    "> - 데이터 `저장공간 확보` 및 `전송효율`과 `연산속도` 유리\n",
    ">\n",
    "> **\"단점\"**\n",
    ">\n",
    "> - 알고리즘으로서 예측하여 `성능 검증`이 어려워 이보다 `데이터 특성 확인`에 많이 사용\n",
    "> - 원데이터 대비 정도의 차이일 뿐 필연적으로 `정보 손실`이 발생\n",
    "> - 변환된 데이터가 `무슨의미인지? 무슨값인지?` 해석의 어려움\n",
    "> - `공분산`이 중요한 기준이기 때문에, `분산이 작은게 좋은 데이터`에서는 `부적합`\n",
    "> - 데이터 분산이 `직교`하지 않으면 부적합 = 데이터 변수가 `독립`이 아니면 부적합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 차원변환 알고리즘의 진화\n",
    "\n",
    "- **(Linear) Projection vs (Non-linear) Manifold Learning:** \n",
    "\n",
    "> | **접근방법** | **알고리즘** |\n",
    "|:---:|---|\n",
    "| **(Linear) Projection** | Eigen Value Decomposition |\n",
    "|  | Singular Value Decomposition |\n",
    "|  | Truncated SVD |\n",
    "|  | Principal Component Analysis |\n",
    "|  | Factor Analysis |\n",
    "|  | Linear Discriminant Analysis |\n",
    "|  | Quadratic Discriminant Analysis |\n",
    "| **(Non-linear) Manifold Learning** | Kernel Principal Component Analysis |\n",
    "|  | Locally Linear Embedding (LLE) |\n",
    "|  | Isomap |\n",
    "|  | Multi Dimensional Scaling |\n",
    "|  | Spectral Embedding |\n",
    "|  | t-distributed Stochastic Neighbor Embedding (t-SNE) |\n",
    "|  | Autoencoders |\n",
    "|  | Self Organizaing Map (SOP) |\n",
    "\n",
    "---\n",
    "\n",
    "- **Comparison:**\n",
    "\n",
    "> **(1)** <center><img src='Image/Advanced/Reduction_Example1.png' width='600'></center>\n",
    "> **(2)** <center><img src='Image/Advanced/Reduction_Example2.png' width='600'></center>\n",
    "> **(3)** <center><img src='Image/Advanced/Reduction_Example3.png' width='600'></center>\n",
    "> **(4)** <center><img src='Image/Advanced/Reduction_Example4.png' width='600'>(https://www.researchgate.net/publication/23953722_Manifold-Based_Learning_and_Synthesis)</center>\n",
    ">\n",
    "> [**(5) Manifold Learning**](https://scikit-learn.org/0.18/modules/manifold.html#manifold)\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "393.021px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
