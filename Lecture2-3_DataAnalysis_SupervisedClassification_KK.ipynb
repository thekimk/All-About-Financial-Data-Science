{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **데이터분석 단계(Data Analysis Cycle)**\n",
    "\n",
    "[![Open in Colab](http://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/thekimk/All-About-Machine-Learning/blob/main/Lecture2-3_DataAnalysis_SupervisedClassification_KK.ipynb)\n",
    "\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle0.png' width='800'></center>\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle1.png' width='800'></center>\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle2.png' width='800'></center>\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle3.png' width='800'></center>\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle4.png' width='800'></center>\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle5.png' width='800'></center>\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle6.png' width='800'></center>\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle7.png' width='800'></center>\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle8.png' width='800'></center>\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle9.png' width='800'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **지도학습(Supervised) 알고리즘:** 분류분석\n",
    "\n",
    "---\n",
    "\n",
    "- **데이터분석 과정:** `학습` + `추론/예측`\n",
    "\n",
    "<center><img src='Image/Advanced/DataSplit_Concept1.png' width='700'></center>\n",
    "<center><img src='Image/Advanced/DataSplit_Concept2.png' width='700'></center>\n",
    "\n",
    "---\n",
    "\n",
    "- **분류분석:** `지도학습` 알고리즘 중 `분류`를 위해 사용되는 가장 `기본(Baseline) 알고리즘`\n",
    "\n",
    "> (비수학적) **\"일상 속 문제 중 여러개의 선택지 중에 정답을 고르는 문제\"**\n",
    "> - 주관식 시험문제의 숫자형 정답을 찾는 문제가 `회귀문제`\n",
    "> - 객관식 시험문제의 정답을 찾는 문제가 `분류문제`\n",
    "\n",
    "> (수학적) **\"범주형 출력(`Y, 종속변수`)에 영향을 주는 입력(`X, 독립변수`)과의 관계를 정량적으로 추론/추정하여 `미래 값을 분류`하는 알고리즘\"**\n",
    "> - **예측문제:** 데이터 변수(Feature, Variable)들을 사용하여 `연속적인(Continuous) 값을 예측`\n",
    "> - **분류문제:** 데이터 변수(Feature, Variable)들을 사용하여 `특정 분류값을 예측`\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<center><img src='Image/Advanced/ML_Type_Application_Upgrade.png' width='700'></center>  \n",
    "<center><img src='Image/Advanced/ML_Type_Category.png' width='700'></center>\n",
    "\n",
    "---\n",
    "\n",
    "| Regression Algorithms | Instance-based Algorithms | Regularization Algorithms | Decision Tree Algorithms | Bayesian Algorithms | Artificial Neural Network Algorithms |\n",
    "|------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| <img src='Image/Advanced/Regression-Algorithms.png' width='150'> | <img src='Image/Advanced/Instance-based-Algorithms.png' width='150'> | <img src='Image/Advanced/Regularization-Algorithms.png' width='150'> | <img src='Image/Advanced/Decision-Tree-Algorithms.png' width='150'> | <img src='Image/Advanced/Bayesian-Algorithms.png' width='150'> | <img src='Image/Advanced/Artificial-Neural-Network-Algorithms.png' width='150'> |\n",
    "| Ordinary Least Squares Regression (OLSR) | k-Nearest Neighbor (kNN) | Ridge Regression | Classification and Regression Tree (CART) | Naive Bayes | Perceptron |\n",
    "| Linear Regression | Learning Vector Quantization (LVQ) | Least Absolute Shrinkage and Selection Operator (LASSO) | Iterative Dichotomiser 3 (ID3) | Gaussian Naive Bayes | Back-Propagation |\n",
    "| Logistic Regression | Self-Organizing Map (SOM) | Elastic Net | C4.5 and C5.0 (different versions of a powerful approach) | Multinomial Naive Bayes | Hopfield Network |\n",
    "| Stepwise Regression | Locally Weighted Learning (LWL) | Least-Angle Regression (LARS) | Chi-squared Automatic Interaction Detection (CHAID) | Averaged One-Dependence Estimators (AODE) | Radial Basis Function Network (RBFN) |\n",
    "| Multivariate Adaptive Regression Splines (MARS) | - | - | Decision Stump | Bayesian Belief Network (BBN) | - |\n",
    "| Locally Estimated Scatterplot Smoothing (LOESS) | - | - | M5 | Bayesian Network (BN) | - |\n",
    "| - | - | - | Conditional Decision Trees | - | - |\n",
    "\n",
    "- **Target Algorithm:**\n",
    "> - `Logistic Regression`\n",
    "> - Ordinal Regression\n",
    "> - Cox Regression\n",
    "> - Naïve Bayes\n",
    "> - Stochastic Gradient Descent\n",
    "> - K-Nearest Neighbours\n",
    "> - Decision Tree\n",
    "> - Random Forest\n",
    "> - Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **예제 데이터셋(Dataset)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## statsmodels 모듈 사용 데이터셋\n",
    "\n",
    "```python\n",
    "# 라이브러리 불러오기\n",
    "import statsmodels.api as sm\n",
    "```\n",
    "\n",
    "> - 대기중 `CO2농도` 데이터:\n",
    "> ```python\n",
    "> data = sm.datasets.get_rdataset(\"CO2\", package=\"datasets\")\n",
    "> ```\n",
    "> - 황체형성 `호르몬(Luteinizing Hormone)`의 수치 데이터:\n",
    "> ```python\n",
    "> data = sm.datasets.get_rdataset(\"lh\")\n",
    "> ```\n",
    "> - 1974~1979년 사이의 영국의 `호흡기 질환 사망자 수` 데이터:\n",
    "> ```python\n",
    "> data = sm.datasets.get_rdataset(\"deaths\", \"MASS\")\n",
    "> ```\n",
    "> - 1949~1960년 사이의 `국제 항공 운송인원` 데이터:\n",
    "> ```\n",
    "> data = sm.datasets.get_rdataset(\"AirPassengers\")\n",
    "> ```\n",
    "> - 미국의 `강수량` 데이터:\n",
    "> ```python\n",
    "> data = sm.datasets.get_rdataset(\"precip\")\n",
    "> ```\n",
    "> - `타이타닉호의 탑승자`들에 대한 데이터:\n",
    "> ```python\n",
    "> data = sm.datasets.get_rdataset(\"Titanic\", package=\"datasets\")\n",
    "> ```\n",
    "\n",
    "- **data가 포함하는 정보:**\n",
    "> - `package`: 데이터를 제공하는 R 패키지 이름\n",
    "> - `title`: 데이터 이름\n",
    "> - `data`: 데이터를 담고 있는 데이터프레임\n",
    "> - `__doc__`: 데이터에 대한 설명 문자열(R 패키지의 내용 기준)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn 모듈 사용 데이터셋\n",
    "\n",
    "**1) 패키지에 포함된 데이터(`load 명령어`)**\n",
    "    \n",
    "```python\n",
    "# 라이브러리 불러오기\n",
    "from sklearn.datasets import load_boston\n",
    "```\n",
    "\n",
    "> - load_boston: 회귀용 `보스턴 집값`\n",
    "> ```python\n",
    "> raw = load_boston()\n",
    "> print(raw.DESCR)\n",
    "> print(raw.keys())\n",
    "> print(raw.data.shape, raw.target.shape)\n",
    "> ```\n",
    "> - load_diabetes: 회귀용 `당뇨병` 자료\n",
    "> - load_linnerud: 회귀용 `linnerud` 자료\n",
    "> - load_iris: 분류용 `붓꽃(iris)` 자료\n",
    "> - load_digits: 분류용 `숫자(digit) 필기 이미지` 자료\n",
    "> - load_wine: 분류용 `포도주(wine) 등급` 자료\n",
    "> - load_breast_cancer: 분류용 `유방암(breast cancer)` 진단 자료\n",
    "\n",
    "**2) 인터넷에서 다운로드할 수 있는 데이터(`fetch 명령어`)**\n",
    "\n",
    "```python\n",
    "# 라이브러리 불러오기\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "```\n",
    "\n",
    "> - fetch_california_housing: : 회귀용 `캘리포니아 집값`\n",
    "> ```python\n",
    "> raw = fetch_california_housing()\n",
    "> print(raw.DESCR)\n",
    "> print(raw.keys())\n",
    "> print(raw.data.shape, raw.target.shape)\n",
    "> ```\n",
    "> - fetch_covtype : 회귀용 `토지` 조사 자료\n",
    "> - fetch_20newsgroups : `뉴스 그룹` 텍스트 자료\n",
    "> - fetch_olivetti_faces : `얼굴 이미지` 자료\n",
    "> - fetch_lfw_people : `유명인 얼굴 이미지` 자료\n",
    "> - fetch_lfw_pairs : `유명인 얼굴 이미지` 자료\n",
    "> - fetch_rcv1 : 로이터 `뉴스 말뭉치`\n",
    "> - fetch_kddcup99 : `Kddcup 99 Tcp dump` 자료\n",
    "\n",
    "**3) 확률분포를 사용한 가상 데이터(`make 명령어`)**\n",
    "\n",
    "```python\n",
    "# 라이브러리 불러오기\n",
    "from sklearn.datasets import make_regression\n",
    "```\n",
    "\n",
    "> - make_regression: `회귀용` 가상 데이터\n",
    "> ```python\n",
    "> X, y, c = make_regression(n_samples=100, n_features=10, n_targets=1, bias=0, noise=0, coef=True, random_state=0)\n",
    "> ```\n",
    "> - make_classification: `분류용` 가상 데이터 생성\n",
    "> - make_blobs: `클러스터링용` 가상 데이터 생성\n",
    "\n",
    "---\n",
    "\n",
    "**4) `load/fetch 명령어 데이터`에서 raw가 포함하는 정보:** Bunch 라는 `클래스 객체 형식`으로 생성\n",
    "\n",
    "> - `data`: (필수) 독립 변수 ndarray 배열\n",
    "> - `target`: (필수) 종속 변수 ndarray 배열\n",
    "> - `feature_names`: (옵션) 독립 변수 이름 리스트\n",
    "> - `target_names`: (옵션) 종속 변수 이름 리스트\n",
    "> - `DESCR`: (옵션) 자료에 대한 설명\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분류문제 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T12:49:07.493271Z",
     "start_time": "2022-06-14T12:49:04.409272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survival of passengers on the Titanic\n",
      ".. container::\n",
      "\n",
      "   ======= ===============\n",
      "   Titanic R Documentation\n",
      "   ======= ===============\n",
      "\n",
      "   .. rubric:: Survival of passengers on the Titanic\n",
      "      :name: survival-of-passengers-on-the-titanic\n",
      "\n",
      "   .. rubric:: Description\n",
      "      :name: description\n",
      "\n",
      "   This data set provides information on the fate of passengers on the\n",
      "   fatal maiden voyage of the ocean liner ‘Titanic’, summarized\n",
      "   according to economic status (class), sex, age and survival.\n",
      "\n",
      "   .. rubric:: Usage\n",
      "      :name: usage\n",
      "\n",
      "   ::\n",
      "\n",
      "      Titanic\n",
      "\n",
      "   .. rubric:: Format\n",
      "      :name: format\n",
      "\n",
      "   A 4-dimensional array resulting from cross-tabulating 2201\n",
      "   observations on 4 variables. The variables and their levels are as\n",
      "   follows:\n",
      "\n",
      "   == ======== ===================\n",
      "   No Name     Levels\n",
      "   1  Class    1st, 2nd, 3rd, Crew\n",
      "   2  Sex      Male, Female\n",
      "   3  Age      Child, Adult\n",
      "   4  Survived No, Yes\n",
      "   == ======== ===================\n",
      "\n",
      "   .. rubric:: Details\n",
      "      :name: details\n",
      "\n",
      "   The sinking of the Titanic is a famous event, and new books are still\n",
      "   being published about it. Many well-known facts—from the proportions\n",
      "   of first-class passengers to the ‘women and children first’ policy,\n",
      "   and the fact that that policy was not entirely successful in saving\n",
      "   the women and children in the third class—are reflected in the\n",
      "   survival rates for various classes of passenger.\n",
      "\n",
      "   These data were originally collected by the British Board of Trade in\n",
      "   their investigation of the sinking. Note that there is not complete\n",
      "   agreement among primary sources as to the exact numbers on board,\n",
      "   rescued, or lost.\n",
      "\n",
      "   Due in particular to the very successful film ‘Titanic’, the last\n",
      "   years saw a rise in public interest in the Titanic. Very detailed\n",
      "   data about the passengers is now available on the Internet, at sites\n",
      "   such as *Encyclopedia Titanica*\n",
      "   (https://www.encyclopedia-titanica.org/).\n",
      "\n",
      "   .. rubric:: Source\n",
      "      :name: source\n",
      "\n",
      "   Dawson, Robert J. MacG. (1995), The ‘Unusual Episode’ Data Revisited.\n",
      "   *Journal of Statistics Education*, **3**.\n",
      "   `doi:10.1080/10691898.1995.11910499 <https://doi.org/10.1080/10691898.1995.11910499>`__.\n",
      "\n",
      "   The source provides a data set recording class, sex, age, and\n",
      "   survival status for each person on board of the Titanic, and is based\n",
      "   on data originally collected by the British Board of Trade and\n",
      "   reprinted in:\n",
      "\n",
      "   British Board of Trade (1990), *Report on the Loss of the ‘Titanic’\n",
      "   (S.S.)*. British Board of Trade Inquiry Report (reprint). Gloucester,\n",
      "   UK: Allan Sutton Publishing.\n",
      "\n",
      "   .. rubric:: Examples\n",
      "      :name: examples\n",
      "\n",
      "   ::\n",
      "\n",
      "      require(graphics)\n",
      "      mosaicplot(Titanic, main = \"Survival on the Titanic\")\n",
      "      ## Higher survival rates in children?\n",
      "      apply(Titanic, c(3, 4), sum)\n",
      "      ## Higher survival rates in females?\n",
      "      apply(Titanic, c(2, 4), sum)\n",
      "      ## Use loglm() in package 'MASS' for further analysis ...\n",
      "\n",
      "dict_keys(['data', '__doc__', 'package', 'title', 'from_cache'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1st</td>\n",
       "      <td>Male</td>\n",
       "      <td>Child</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2nd</td>\n",
       "      <td>Male</td>\n",
       "      <td>Child</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3rd</td>\n",
       "      <td>Male</td>\n",
       "      <td>Child</td>\n",
       "      <td>No</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crew</td>\n",
       "      <td>Male</td>\n",
       "      <td>Child</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1st</td>\n",
       "      <td>Female</td>\n",
       "      <td>Child</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2nd</td>\n",
       "      <td>Female</td>\n",
       "      <td>Child</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3rd</td>\n",
       "      <td>Female</td>\n",
       "      <td>Child</td>\n",
       "      <td>No</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Crew</td>\n",
       "      <td>Female</td>\n",
       "      <td>Child</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1st</td>\n",
       "      <td>Male</td>\n",
       "      <td>Adult</td>\n",
       "      <td>No</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2nd</td>\n",
       "      <td>Male</td>\n",
       "      <td>Adult</td>\n",
       "      <td>No</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3rd</td>\n",
       "      <td>Male</td>\n",
       "      <td>Adult</td>\n",
       "      <td>No</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Crew</td>\n",
       "      <td>Male</td>\n",
       "      <td>Adult</td>\n",
       "      <td>No</td>\n",
       "      <td>670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1st</td>\n",
       "      <td>Female</td>\n",
       "      <td>Adult</td>\n",
       "      <td>No</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2nd</td>\n",
       "      <td>Female</td>\n",
       "      <td>Adult</td>\n",
       "      <td>No</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3rd</td>\n",
       "      <td>Female</td>\n",
       "      <td>Adult</td>\n",
       "      <td>No</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Crew</td>\n",
       "      <td>Female</td>\n",
       "      <td>Adult</td>\n",
       "      <td>No</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1st</td>\n",
       "      <td>Male</td>\n",
       "      <td>Child</td>\n",
       "      <td>Yes</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2nd</td>\n",
       "      <td>Male</td>\n",
       "      <td>Child</td>\n",
       "      <td>Yes</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3rd</td>\n",
       "      <td>Male</td>\n",
       "      <td>Child</td>\n",
       "      <td>Yes</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Crew</td>\n",
       "      <td>Male</td>\n",
       "      <td>Child</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1st</td>\n",
       "      <td>Female</td>\n",
       "      <td>Child</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2nd</td>\n",
       "      <td>Female</td>\n",
       "      <td>Child</td>\n",
       "      <td>Yes</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3rd</td>\n",
       "      <td>Female</td>\n",
       "      <td>Child</td>\n",
       "      <td>Yes</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Crew</td>\n",
       "      <td>Female</td>\n",
       "      <td>Child</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1st</td>\n",
       "      <td>Male</td>\n",
       "      <td>Adult</td>\n",
       "      <td>Yes</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2nd</td>\n",
       "      <td>Male</td>\n",
       "      <td>Adult</td>\n",
       "      <td>Yes</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3rd</td>\n",
       "      <td>Male</td>\n",
       "      <td>Adult</td>\n",
       "      <td>Yes</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Crew</td>\n",
       "      <td>Male</td>\n",
       "      <td>Adult</td>\n",
       "      <td>Yes</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1st</td>\n",
       "      <td>Female</td>\n",
       "      <td>Adult</td>\n",
       "      <td>Yes</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2nd</td>\n",
       "      <td>Female</td>\n",
       "      <td>Adult</td>\n",
       "      <td>Yes</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3rd</td>\n",
       "      <td>Female</td>\n",
       "      <td>Adult</td>\n",
       "      <td>Yes</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Crew</td>\n",
       "      <td>Female</td>\n",
       "      <td>Adult</td>\n",
       "      <td>Yes</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class     Sex    Age Survived  Freq\n",
       "0    1st    Male  Child       No     0\n",
       "1    2nd    Male  Child       No     0\n",
       "2    3rd    Male  Child       No    35\n",
       "3   Crew    Male  Child       No     0\n",
       "4    1st  Female  Child       No     0\n",
       "5    2nd  Female  Child       No     0\n",
       "6    3rd  Female  Child       No    17\n",
       "7   Crew  Female  Child       No     0\n",
       "8    1st    Male  Adult       No   118\n",
       "9    2nd    Male  Adult       No   154\n",
       "10   3rd    Male  Adult       No   387\n",
       "11  Crew    Male  Adult       No   670\n",
       "12   1st  Female  Adult       No     4\n",
       "13   2nd  Female  Adult       No    13\n",
       "14   3rd  Female  Adult       No    89\n",
       "15  Crew  Female  Adult       No     3\n",
       "16   1st    Male  Child      Yes     5\n",
       "17   2nd    Male  Child      Yes    11\n",
       "18   3rd    Male  Child      Yes    13\n",
       "19  Crew    Male  Child      Yes     0\n",
       "20   1st  Female  Child      Yes     1\n",
       "21   2nd  Female  Child      Yes    13\n",
       "22   3rd  Female  Child      Yes    14\n",
       "23  Crew  Female  Child      Yes     0\n",
       "24   1st    Male  Adult      Yes    57\n",
       "25   2nd    Male  Adult      Yes    14\n",
       "26   3rd    Male  Adult      Yes    75\n",
       "27  Crew    Male  Adult      Yes   192\n",
       "28   1st  Female  Adult      Yes   140\n",
       "29   2nd  Female  Adult      Yes    80\n",
       "30   3rd  Female  Adult      Yes    76\n",
       "31  Crew  Female  Adult      Yes    20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Classification\n",
    "import statsmodels.api as sm\n",
    "data = sm.datasets.get_rdataset(\"Titanic\", package=\"datasets\")\n",
    "print(data.title)\n",
    "print(data.__doc__)\n",
    "print(data.keys())\n",
    "display(data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T12:49:07.598303Z",
     "start_time": "2022-06-14T12:49:07.495275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry\n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        worst/largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "        10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n",
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n",
      "(569, 30) (569,)\n"
     ]
    }
   ],
   "source": [
    "# Classification\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "raw = load_breast_cancer()\n",
    "print(raw.DESCR)\n",
    "print(raw.keys())\n",
    "print(raw.data.shape, raw.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T12:49:07.613302Z",
     "start_time": "2022-06-14T12:49:07.600272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10) (100,)\n"
     ]
    }
   ],
   "source": [
    "# Classification\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=100, n_features=10, n_classes=2,\n",
    "                           n_informative=5, n_redundant=0, random_state=0)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **전처리 방향(Preprocessing)**\n",
    "\n",
    "- **목표:** \n",
    "> - 대량으로 수집된 데이터는 `그대로 활용 어려움`\n",
    "> - `잘못 수집/처리 된 데이터`는 엉뚱한 결과를 발생\n",
    "> - 알고리즘이 `학습이 가능한 형태`로 데이터를 정리\n",
    "<center><img src='Image/Advanced/DataAnalysis_Time.jpg' width='500'></center> \n",
    "---\n",
    "\n",
    "> **일반적인 전처리 필요항목:**  \n",
    "> - 데이터 결합\n",
    "> - 결측값 처리\n",
    "> - 이상치 처리\n",
    "> - 자료형 변환\n",
    "> - 데이터 분리\n",
    "> - 데이터 변환\n",
    "> - 스케일 조정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **함수세팅 및 추정 방향(Modeling):** Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분류문제에 회귀분석 사용시 한계 및 대응"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) 회귀문제에 회귀분석은 적절**\n",
    "\n",
    "> **\"연속형 종속변수 예시\"**\n",
    ">\n",
    "> <center><img src='Image/Advanced/Example_LinearRegression.png' width='600'></center>\n",
    "\n",
    "**2) 분류문제에 회귀분석은 부적절**\n",
    "\n",
    "> **\"범주형 종속변수 예시\"**\n",
    "> - Outlier가 존재하면 Linear Regression의 추정은 왜곡을 발생시킴\n",
    ">\n",
    "> <center><img src='Image/Advanced/Example_LinearRegression_Limit.png' width='600'></center>\n",
    "\n",
    "**3) 분류문제 해결을 위한 대응:**\n",
    "\n",
    "> **회귀분석:** `연속형 종속변수 Y`의 값을 추론\n",
    "> - 혈압의 경우 값 자체로 의미가 있지만 `암발생은 발생(1)과 미발생(0) 사이의 중간값 무의미`\n",
    "> - 회귀분석으로 범주형 종속변수를 추론하면 범위가 맞지 않아 `암발생 여부의 해석이 왜곡`\n",
    ">> - 나이가 많아지면 무조건 암이 걸리거나 나이가 어리면 무조건 암이 걸리지 않는 왜곡 발생\n",
    "> - 범주형 Y일 경우 회귀분석의 한계 존재하며 이를 해결하기 위한 접근 필요\n",
    "\n",
    "> **분류분석:** `범주형(범주/카테고리/클래스/라벨) 종속변수 Y`의 분류를 추론\n",
    "> - **시그모이드 함수(Sigmoid Function) 적용:** 연속형 Y를 `0과 1사이의 값으로 변환`하면서 분류에 맞게 `S자 형태로 적합(Fitting)`하는 함수\n",
    ">\n",
    "> <center><img src='Image/Advanced/Example_LogisticRegression.png' width='600'></center>\n",
    "\n",
    "**4) 분류분석 종류:** Y 카테고리 갯수와 방향에 따라 `Binary/Multi-class/Multi-label`로 구분\n",
    "> - **Binary Classification:** 데이터가 `2개의 카테고리` 중 `어떤 것`인지 추론하는 문제 (ex. 성별 추론 / 스팸메일 추론)\n",
    "> - **Multi-class Classification:** 데이터가 `2개 이상의 카테고리` 중 `어떤 것`인지 추론하는 문제 (ex. 사진으로 동물 이름 추론)\n",
    "> - **Multi-label Classification:** 데이터가 `2개 이상의 카테고리` 중 `어떠한 것들`인지 추론하는 문제 (ex. 뉴스기사는 스포츠/사람/지역 관련임을 추론)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분류문제 해결을 위한 가설 및 비용함수\n",
    "\n",
    "> **\"선형회귀분석을 포함하여 머신러닝과 딥러닝 등의 `모든 알고리즘은 큰 틀에서 작동방식이 동일`\"**\n",
    ">\n",
    "> **\"머신러닝과 딥러닝의 작동방식을 이해하기 위해 가장 기초 예측 알고리즘인 `선형회귀분석(Linear Regression)` 작동방식부터\"**\n",
    ">\n",
    "> **\"`선형회귀분석`을 포함한 대부분의 알고리즘은 큰 틀에서 `3가지 도구`를 사용하여 작동\"**\n",
    ">\n",
    "> **1)** `방정식(Equation) = 함수(Function) = 가설(Hypothesis)`\n",
    ">\n",
    "> **2)** `비용함수(Cost Function)`\n",
    ">\n",
    "> **3)** `옵티마이저(Optimizer)`\n",
    "\n",
    "---\n",
    "\n",
    "**1-1) 알고리즘 함수세팅:** 분류문제를 푸는 대표적인 알고리즘 `Logistic Regression`\n",
    "\n",
    "- `범주형 종속변수`의 적합/추정하기 위한 `변환과정` 필요\n",
    "- `Logistic/Sigmoid Function`를 사용하여 `곡선(S-curve) 형태로 변환`\n",
    "\n",
    "<center><img src='Image/Advanced/Linear_Logistic.png' width='600'></center>\n",
    "\n",
    "> **(1) 회귀분석 추정:**\n",
    ">\n",
    ">\\begin{align*}\n",
    "Y \\approx \\hat{Y} &= f(X_1, X_2, ..., X_k) \\\\\n",
    "&= w_0 + w_1X_1 + w_2X_2 + \\cdots + w_kX_k \\\\\n",
    "&= XW\n",
    "\\end{align*}\n",
    ">\n",
    "> **(2) 시그모이드 변환(Logistic/Sigmoid Transformation):** `Binary Classification` 반영하는 `곡선 형태`로 변경\n",
    ">\n",
    ">\\begin{align*}\n",
    "Pr(\\hat{Y}) &= \\dfrac{1}{1+exp(-\\hat{Y})} \\\\ \n",
    "&= \\dfrac{1}{1+exp(-XW)} \\\\\n",
    "&= \\dfrac{exp(XW)}{1+exp(XW)}\n",
    "\\end{align*}\n",
    ">\n",
    "> **(3) 로짓 변환(Logit Transformation):** `X`의 선형관계 형태로 변환하여 `변수들`로 `Y=1`인 확률 추정\n",
    ">\n",
    ">\\begin{align*}\n",
    "Pr(\\hat{Y}) \\left( 1 + exp(XW) \\right) &= exp(XW) \\\\\n",
    "Pr(\\hat{Y}) &= \\left( 1 - Pr(\\hat{Y}) \\right) exp(XW) \\\\\n",
    "\\text{Odds(ratio):} \\left( \\dfrac{Pr(\\hat{Y})}{1 - Pr(\\hat{Y})} \\right) &= exp(XW) \\\\\n",
    "\\text{Logit(log-odds): } log \\left( \\dfrac{Pr(\\hat{Y})}{1 - Pr(\\hat{Y})} \\right) &= XW = w_0 + w_1X_1 + w_2X_2 + \\cdots + w_kX_k \\\\\n",
    "\\end{align*}\n",
    "\n",
    "---\n",
    "\n",
    "**1-2) 추정 결과 해석:**\n",
    "\n",
    "> **(1) 해석 방향:** $\\hat{Logit}$과 $\\hat{Odds}$ `변환`으로 가능\n",
    "> $$\n",
    "\\begin{align*}\n",
    "\\text{Logit: } log \\left( \\dfrac{Pr(Y)}{1 - Pr(Y)} \\right) &= X\\hat{W} = \\hat{w}_0 + \\hat{w}_1X_1 + \\hat{w}_2X_2 + \\cdots + \\hat{w}_kX_k \\\\\n",
    "\\text{Odds: } \\left( \\dfrac{Pr(Y)}{1 - Pr(Y)} \\right) &= exp(X\\hat{W}) = exp(\\hat{w}_0 + \\hat{w}_1X_1 + \\hat{w}_2X_2 + \\cdots + \\hat{w}_kX_k) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    ">\n",
    "> **(2) 회귀분석과 달리 `Y`의 로짓변환 값을 `X`의 선형관계로 추정하기 때문에, 해석시 `Odds`로 변환해서 해석해야 하므로 `주의`**\n",
    ">\n",
    "> \\begin{align*}\n",
    "\\left( \\dfrac{Pr(Y)}{1 - Pr(Y)} \\right) = exp(0.01 + 0.8 X_1)\n",
    "\\end{align*}\n",
    ">\n",
    "> - **선형회귀분석:** $X_1$이 1만큼 증가하면 $Y$는 $w_1$만큼 증가\n",
    ">\n",
    ">> : $X_1$이 `1`만큼 증가하면 $Y$는 `0.8`만큼 증가\n",
    ">\n",
    "> - **로지스틱회귀분석:** $X_1$이 1만큼 증가하면 $\\left( \\dfrac{Pr(\\hat{Y})}{1 - Pr(\\hat{Y})} \\right)$ 범주변화는 $exp(w_1)$만큼 증가\n",
    ">\n",
    ">> : $X_1$이 `1`만큼 증가하면 암에 걸리지 않을 확률보다 암에 걸릴 확률이 $exp(0.8)$ = `2.23`배 더 높음\n",
    ">\n",
    "> **(3) Y 확률 예측:** `추정된 계수`의 함수를 `로지스틱 변환`으로 출력\n",
    ">\n",
    "> \\begin{align*}\n",
    "Pr(\\hat{Y}) &= \\dfrac{1}{1+exp(-X\\hat{W})} = \\dfrac{exp(X\\hat{W})}{1+exp(X\\hat{W})}\n",
    "\\end{align*}\n",
    ">\n",
    "> **(4) 분류 의사결정:** 기본 임계값은 `0.5`로 Y 확률 예측 값이 `0.5 이상이면 1`, `0.5 미만이면 0`으로 분류\n",
    ">\n",
    "> \\begin{align*}\n",
    "\\hat{Y} = \\begin{cases} 1 ~~~~ \\text{if } ~~~ Pr(\\hat{Y}) >= 0.5 \\\\ 0 ~~~~ \\text{if } ~~~ Pr(\\hat{Y}) < 0.5 \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "---\n",
    "\n",
    "**2) 함수 추정을 위한 비용함수:** 나의 주장 기반 알고리즘의 `분류값`($Pr(\\hat{Y})$)과 `실제 데이터`($Y$)의 차이를 평가하는 함수\n",
    "\n",
    "- **이슈: `잔차`를 사용하는 Linear Regression 비용함수 적용 어려움**    \n",
    "\n",
    "> (1) 분류문제에서는 $\\hat{Y}$를 사용한 `잔차(에러)계산이 무의미`   \n",
    ">\n",
    "> (2) 잔차($Y - \\hat{Y}$)를 `시그모이드 및 로짓 변환`을 하면 Non-convex 형태가 되서 `최소값(Global Minimum) 추정 어려움`    \n",
    ">\n",
    "> (3) 정확한 `수학적 방정식 기반` 계수추정 어렵기에 `확률론적 접근 필요`    \n",
    ">\n",
    "> <center><img src='Image/Advanced/Cost_Comparison.png' width='600'></center>\n",
    "\n",
    "- **방향:** 회귀문제와 달리 `새로운 비용함수`가 필요\n",
    "\n",
    "> - Y를 `잘` 분류하면 `cost=0`으로 그렇지 않으면 cost=$\\infty$가 되는 방향\n",
    ">> - (빨간선) 실제값이 `1`일때 예측값이 `1`이면 Cost는 `0`\n",
    ">> - (빨간선) 실제값이 `1`일때 예측값이 `0`이면 Cost는 `무한대`\n",
    ">\n",
    "> \\begin{align*}\n",
    "\\text{Cost} = \\begin{cases} -log(Pr(\\hat{Y})) ~~~~ & \\text{in the case of } ~~~ Y = 1 \\\\ -log(1-Pr(\\hat{Y})) ~~~~ & \\text{in the case of } ~~~ Y = 0 \\end{cases}\n",
    "\\end{align*}\n",
    "> <center><img src='Image/Advanced/Cost_Logistic.png' width='600'></center>\n",
    "\n",
    "- **Cross Entropy 등장:** Y가 0과 1인 경우의 `Cost를 결합`하여 하나의 식으로 표현\n",
    "\n",
    "> - 로지스틱 알고리즘은 `비용함수`로 `Cross Entropy`를 사용하고 `최소로 하는 계수/가중치 추정`\n",
    "> - `Y=0`인 경우 `파란부분`만 남고 `Y=1`인 경우 `빨간부분`만 남아 `Class별`로 독립적으로 작동\n",
    "> - 분류문제의 `Cost 함수는 다양`하고 많지만 통계학적으로 `Cross Entropy`는 계수 추정에 `효율적`인 편\n",
    "> - `Convex 형태`이기 때문에 `Global Minimum`을 찾기가 용이함\n",
    "> - 추정된 계수/가중치($\\hat{w}$)로 방정식을 만들어 Y=1인 `분류확률` 계산 가능\n",
    ">\n",
    ">\\begin{align*}\n",
    "\\text{Cost} &= \\sum_{i=1}^{m} \\left[ - \\color{red}{\\hat{Y}_{i} log (Pr(\\hat{Y}_{i}))} - \\color{blue}{(1-\\hat{Y}_{i}) log (1-Pr(\\hat{Y}_{i}))} \\right] \\\\\n",
    "\\hat{W} &= \\underset{W}{\\arg\\min} \\sum_{i=1}^{m} \\left[\\text{Cost} \\right] \\\\\n",
    "\\end{align*}\n",
    "\n",
    "---\n",
    "\n",
    "- **Comparison Summary:**\n",
    "\n",
    "| **구분** | **Linear Regression** | **Logistic Regression** |\n",
    "|:---:|:---:|:---:|\n",
    "| **문제 유형<br>(해결 방향)** | 회귀<br>(Regression) | 분류<br>(Classification) |\n",
    "| **모델 가정<br>(전문가 이상형)** | 직선<br>(Linear) | 곡선<br>(Sigmoid) |\n",
    "| **비용 함수<br>(평가 기준)** | 정답과 예측의 차이가 최소인 직선 선택<br>(MSE) | 각 라벨을 잘 맞춘 곡선 선택<br>(Entropy) |\n",
    "| **검증 지표<br>(평가 확대)** | 정답과 예측의 차이가 얼마나 최소인가?<br>(RMSE, MSPE, MAE, MAPE, MedAE, MedAPE 등) | 각 라벨을 얼마나 잘 맞췄나?<br>(Precision, Recall, F1-score, Accuracy, AUC, ROC 등) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **확률론적 모형(Probabilistic Model):** 통계적 모형\n",
    "> **\"종속변수의 발생가능성을 최대(최소)로하는 $W$를 추정\"**\n",
    "> - 범주형 `분류문제`를 `확률`로 반영하였기 때문에 `확률론적 방식`으로 접근\n",
    "\n",
    "**1) 실제 Y값의 추정가능성(Likelihood):** \n",
    "\n",
    "\\begin{align*}\n",
    "Pr(Y_{i} \\,\\big|\\, X_{i}) &= \\prod_{i=1}^m Pr(Y_{i})^{Y_{i}} [1-Pr(Y_{i})]^{1-Y_{i}}\n",
    "\\end{align*}\n",
    "\n",
    "**2) 추정가능성의 더하기 표시 변환을 위한 Log함수 적용(Log-Likelihood):** \n",
    "\n",
    "\\begin{align*}\n",
    "\\text{LL} &= \\log Pr(Y_{i} \\,\\big|\\, X_{i}) \\\\\n",
    "&= \\sum_{i=1}^{m} \\left[ Y_{i} log (Pr(Y_{i})) + (1-Y_{i}) log (1-Pr(Y_{i})) \\right]\n",
    "\\end{align*}\n",
    "\n",
    "**3) Log-Likelihood의 음수화 및 그레디언트가 `최소`가 되는 곳:**\n",
    "\n",
    "\\begin{align*}\n",
    "- \\dfrac{d}{dW} \\text{LL} &= \\text{minimum} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "**4) `수치해석 방법론`으로 초기값 $W$의 반복적 업데이트를 통한 최종 $\\hat{W}$ 추정:**\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{W}^{new} &= \\hat{W}^{old} - (\\dfrac{d^2}{dW dW^T} \\text{LL})^{-1} \\dfrac{d}{dW} \\text{LL}\n",
    "\\end{align*}\n",
    "\n",
    "> (1) 계수에 임의의 초기값을 반영하여 Logit 추정   \n",
    ">\n",
    "> $$\n",
    "\\begin{align*}\n",
    "\\text{Logit: } log \\left( \\dfrac{Pr(Y)}{1 - Pr(Y)} \\right) &= X\\hat{W} = \\hat{w}_0 + \\hat{w}_1X_1 + \\hat{w}_2X_2 + \\cdots + \\hat{w}_kX_k\n",
    "\\end{align*}\n",
    "$$\n",
    ">\n",
    "> (2) 추정된 Logit으로 Likelihood기반 비용함수 계산 \n",
    ">\n",
    "> $$\n",
    "\\begin{align*}\n",
    "\\text{LL} &= \\log Pr(Y_{i} \\,\\big|\\, X_{i}) \\\\\n",
    "&= \\sum_{i=1}^{m} \\left[ Y_{i} log (Pr(Y_{i})) + (1-Y_{i}) log (1-Pr(Y_{i})) \\right]\n",
    "\\end{align*}\n",
    "$$\n",
    ">\n",
    "> (3) 비용함수를 감소시키는 방향으로 $\\hat{W}$를 업데이트하며 최적화\n",
    ">\n",
    "> $$\n",
    "\\begin{align*}\n",
    "\\hat{W}^{new} &= \\hat{W}^{old} - (\\dfrac{d^2}{dW dW^T} \\text{LL})^{-1} \\dfrac{d}{dW} \\text{LL}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class 분류문제\n",
    "\n",
    "- **Binary vs. Multi-class:**\n",
    "\n",
    "<center><img src='Image/Advanced/Classification_BinaryMuticlass.png' width='600'></center>\n",
    "\n",
    "- **방향:** `N개`의 Class(Category)가 있는 문제는 `N개의 Binary Classification`으로 바꾸어 해결\n",
    "\n",
    "> (1) 세모가 Positive일때 Y가 세모에 속할 확률\n",
    ">\n",
    "> (2) 네모가 Positive일때 Y가 네모에 속할 확률\n",
    ">\n",
    "> (3) 엑스가 Positive일때 Y가 엑스에 속할 확률\n",
    ">\n",
    "> **\"데이터가 주어지면 3개의 경우를 모두 적용하여 최대 확률을 갖는 Class로 추정\"**\n",
    ">\n",
    "> <center><img src='Image/Advanced/Classification_Multiclass.png' width='600'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **검증지표 방향(Evaluation Metrics)**\n",
    "\n",
    "<center><img src='Image/Advanced/Analysis_Process.png' width='800'></center>\n",
    "\n",
    "> **\"`문제해결 검증지표`와 `알고리즘 검증지표`는 같을 수 있으나 대부분은 다른 편\"**\n",
    ">\n",
    "> **(1) 문제해결 검증지표:** `실제 문제`를 잘 해결하는지 평가 `(3단계)`\n",
    ">\n",
    "> **(2) 알고리즘 검증지표:** `데이터의 패턴`이 잘 추출되고 `예측의 정확성`을 평가 `(2단계)`\n",
    ">\n",
    "> - `알고리즘 성능`이 좋은것과 `문제해결`이 가능한 것은 다르기 때문에 문제해결 지표와 알고리즘 지표는 대부분은 다른 편\n",
    "> - 알고리즘 검증지표는 없어도 되지만 `문제해결 검증지표는 반드시 필요`    \n",
    "> - `(이론적)알고리즘`들은 일반적으로 특정 `알고리즘 검증지표`를 향상시키는 방향으로 개발됨\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 대표적인 검증지표\n",
    "\n",
    "---\n",
    "\n",
    "<center><img src='Image/Advanced/DataSplit_Concept1.png' width='700'></center>\n",
    "<center><img src='Image/Advanced/DataSplit_Concept2.png' width='700'></center>\n",
    "\n",
    "---\n",
    "\n",
    "**1) 문제별 종류:**\n",
    "\n",
    "<center><img src='Image/Advanced/Evaluation_Metric_Types.png' width='600'></center>\n",
    "\n",
    "> - **Statistical Metrics:** `Correlation`\n",
    ">> - 입력(Input): `-무한대 ~ 무한대` 범위의 연속형 값\n",
    ">> - 출력(Output): 이론적으론 `-1 ~ 1` 범위의 연속형 값\n",
    "> - **Regression Metrics:** `MSE, MSPE, RMSE, RMSLE, MAE, MAPE, MPE, R^2, Adjusted R^&2, ...` (Y의 범위가 무한대가 가능한 연속형일때)\n",
    ">> - 입력(Input): `-무한대 ~ 무한대` 범위의 연속형 값\n",
    ">> - 출력(Output): 이론적으론 `0 ~ 무한대` 범위의 연속형 값\n",
    "> - **Classification Metrics:** `Log Loss, Cross-entropy, ROC, AUC, Gini, Confusion Matrix, Accuracy, Precision, Recall, F1-score, Classification Report, KS Statistic, Concordant-Discordant Ratio, (ARI, NMI, AMI), ...` (Y가 2개 또는 그 이상개수의 이산형일때)\n",
    ">> - 입력(Input): `-무한대 ~ 무한대` 범위의 연속형 값\n",
    ">> - 출력(Output): 알고리즘 종류에 따라 출력이 달라질 수 있음\n",
    ">>> - 확률(Probability): `0 ~ 1` 범위의 연속형 값 (Logistic Regression, Random Forest, Gradient Boosting, Adaboost, ...)\n",
    ">>> - 집단(Class): `0 또는 1`의 이산형 값 (SVM, KNN, ...)\n",
    "> - **Clustering:** `Dunn Index, Silhouette, ...`\n",
    "> - **Ranking Metrics:** `Gain, Lift, MRR, DCG, NDCG, ...`\n",
    "> - **Computer Vision Metrics:** `PSNR, SSIM, IoU, ...`\n",
    "> - **NLP Metrics:** `Perplexity, BLEU score, ...`\n",
    "> - **Deep Learning Related Metrics:** `Inception score, Frechet Inception distance, ...`\n",
    "> - **Real Problem:** `???`\n",
    "\n",
    "---\n",
    "\n",
    "**2) 검증지표 성능의 종류:** 데이터/분석은 `높은 정확도`를 낳거나 `높은 에러`를 발생시킴\n",
    "> - **높은정확도(High Accuracy):** `과거 패턴`이 미래에도 그대로 유지가 된다면 예측 정확도가 높아짐  \n",
    "> - **높은에러(High Error):** `패턴이 점차적으로 또는 갑자기 변경되면` 예측값은 실제값에서 크게 벗어날 수 있음  \n",
    ">> - **Black Swan:** <U>일어날 것 같지 않은 일이 일어나는 현상</U>\n",
    ">> - **White Swan:** <U>과거 경험들로 충분히 예상되는 위기지만 대응책이 없고 반복될 현상</U>\n",
    ">> - **Gray Swan:** <U>과거 경험들로 충분히 예상되지만 발생되면 충격이 지속되는 현상</U>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분류분석 검증지표 및 해석하기\n",
    "\n",
    "---\n",
    "- **Structure:**\n",
    "\n",
    "<center><img src='Image/Advanced/DataSplit_Concept1.png' width='700'></center>\n",
    "\n",
    "<center><img src='Image/Advanced/Evaluation_Yhat.PNG' width='600'></center>\n",
    "\n",
    "---\n",
    "\n",
    "**1) 오차행렬(Confusion Matrix):** `정답 클래스`와 `알고리즘 예측 클래스`의 일치 갯수 정리  \n",
    "\n",
    "- `Binary` Classification\n",
    "\n",
    "| 　 | 예측 0 | 예측 1 |\n",
    "|:-:|:-:|:-:|\n",
    "| 정답 0 | 정답이 0, 예측이 0인 데이터 수 <br> (3) | 정답이 0, 예측이 1인 데이터 수 <br> (0)|\n",
    "| 정답 1 | 정답이 1, 예측이 0인 데이터 수 <br> (1) | 정답이 1, 예측이 1인 데이터 수 <br> (3) |\n",
    "\n",
    "- `Multi-class` Classification\n",
    "\n",
    "| 　 | 예측 0 | 예측 1 | … | 예측 K |\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "| 정답 0 | 정답 0, 예측 0인 데이터 수 | 정답 0, 예측 1인 데이터 수 | … | 정답 0, 예측 K인 데이터 수 |\n",
    "| 정답 1 | 정답 1, 예측 0인 데이터 수 | 정답 1, 예측 1인 데이터 수 | … | 정답 1, 예측 K인 데이터 수 |\n",
    "| … | … | … | … | … |\n",
    "| 정답 K | 정답 K, 예측 0인 데이터 수 | 정답 K, 예측 1인 데이터 수 | … | 정답 K, 예측 K인 데이터 수 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) 정확도(Accuracy):** 전체 데이터 중 정확하게 예측한 클래스의 비율(`0과 1을 모두 포함`)\n",
    "> - 예측이 정답과 얼마나 정확한가?\n",
    "\n",
    "| 　 | 예측 0 | 예측 1 |\n",
    "|:-:|:-:|:-:|\n",
    "| 정답 0 | True Negative (TN) <br> (3) | False Positive (FP) <br> (0) |\n",
    "| 정답 1 | False Negative (FN) <br> (1) | True Positive (TP) <br> (3) |\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + FN + FP + TN}\n",
    "\\end{align*}\n",
    "\n",
    "**3) 정밀도(Precision):** `클래스 1`로 예측한 값들 중 `실제 클래스 1의 비율`\n",
    "> - 예측한 것중 정답의 비율은?\n",
    "> - `잘못예측한 클래스 1의 비중`을 파악하고 줄이는데 목적\n",
    "> - `암환자`가 아닌데 암에 걸릴거라고 예측하여 과도한 사람들의 `검진 증가 우려`\n",
    "\n",
    "| 　 | 예측 1 |\n",
    "|:-:|:-:|\n",
    "| 정답 0 | False Positive (FP) <br> (0) |\n",
    "| 정답 1 | True Positive (TP) <br> (3) |\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "\\end{align*}\n",
    "\n",
    "**4) 재현율(Recall/Sensitivity/True Positive Rate):** `실제 클래스 1` 값들 중 `예측 클래스 1`의 비율\n",
    "> - 정답 클래스 1중 예측으로 맞춘 비율은?\n",
    "> - `잘못예측한 클래스 0의 비중`을 파악하고 줄이는데 목적\n",
    "> - `암환자`인데 암이 아니라 예측하여 과도한 `사망율 증가 우려`\n",
    "\n",
    "| 　 | 예측 0 | 예측 1 |\n",
    "|:-:|:-:|:-:|\n",
    "| 정답 1 | False Negative (FN) <br> (1) | True Positive (TP) <br> (3) |\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Recall = TPR} = \\frac{TP}{TP + FN}\n",
    "\\end{align*}\n",
    "\n",
    "**5) F1점수(F1-score):** `정밀도`와 `재현율`의 `Trade Off`관계 반영위해 `(가중)평균`으로 모두 잘 맞추었는지 평가\n",
    "> - 정밀도과 재현율이 `모두 중요한 문제`의 경우 중요\n",
    "> - 정밀도와 재현율을 `따로 보면 한쪽으로 편중된(Bias) 의사결정`이 될 수 있는 위험\n",
    "> - 다양한 평균의 종류 중 `조화평균`을 사용하여 계산\n",
    "> - 정밀도과 재현율 중 `한쪽에 치우치지 않았을 때 높은 값`\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{F1-score} = \\frac{2 * precision * recall}{precision + recall}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6) ROC커브(Receiver Operator Characteristic Curve):** 분류 기준값(Threshold)에 따라 `재현율과 거짓율의 검증지표의 변화`를 확인하기 위한 `시각화 지표`\n",
    "\n",
    "<center><img src='Image/Advanced/Evaluation_Yhat.PNG' width='600'></center>\n",
    "\n",
    "- **실제 예측결과 분포(히스토그램):**\n",
    "\n",
    "<center><img src='Image/Advanced/Evaluation_Prediction_Hist.png' width='500'></center>\n",
    "\n",
    "<center><img src='Image/Advanced/Evaluation_ROC_Distribution.png' width='500'></center>\n",
    "\n",
    "| 　 | 예측 0 | 예측 1 |\n",
    "|:-:|:-:|:-:|\n",
    "| 정답 0 | True Negative (TN) <br> (3) | False Positive (FP) <br> (0) |\n",
    "| 정답 1 | False Negative (FN) <br> (1) | True Positive (TP) <br> (3) |\n",
    "\n",
    "- **재현율(Recall/Sensitivity/True Positive Rate):** `실제 클래스 1` 값들 중 `예측 클래스 1`의 비율\n",
    "> - 정답 클래스 1중 예측으로 `맞춘 비율`은?\n",
    "> - `잘못예측한 클래스 0의 비중`을 파악하고 줄이는데 목적\n",
    "> - `암환자`인데 암이 아니라 예측하여 과도한 `사망율 증가 우려`\n",
    "\n",
    "| 　 | 예측 0 | 예측 1 |\n",
    "|:-:|:-:|:-:|\n",
    "| 정답 1 | False Negative (FN) <br> (1) | True Positive (TP) <br> (3) |\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{TPR} = \\frac{TP}{TP + FN}\n",
    "\\end{align*}\n",
    "\n",
    "- **거짓율(Fall-out/False Positive Rate):** `실제 클래스 0` 값들 중 `예측 클래스 1`의 비율\n",
    "> - 정답 클래스 0중 예측으로 `틀린 비율`은?\n",
    "> - 다른 Metrics와 달리 `낮을 수록 좋음`\n",
    "\n",
    "| 　 | 예측 0 | 예측 1 |\n",
    "|:-:|:-:|:-:|\n",
    "| 정답 0 | True Negative (TN) <br> (3) | False Positive (FP) <br> (0) |\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{FPR} = \\frac{FP}{FP + TN}\n",
    "\\end{align*}\n",
    "\n",
    "- **RUC Curve:** 재현율과 거짓율의 `변화를 시각화`\n",
    "\n",
    "> **기준값 변화에 따른 변화:**\n",
    ">\n",
    "> - 재현율(TPR)과 거짓율(FPR)은 `양의 상관관계` 존재\n",
    "> - Threshold가 낮아지면 `1예측 갯수가 많아지고 TP와 FP모두 증가`\n",
    "> $\\rightarrow$  `TPR & FPR 증가`\n",
    "> - Threshold가 높아지면 `1예측 갯수가 줄어들고 TP와 FP모두 감소`\n",
    "> $\\rightarrow$  `TPR & FPR 감소`\n",
    "\n",
    "<center><img src='Image/Advanced/TPR_FPR_Positive.gif' width='400'></center>\n",
    "\n",
    "<center><img src='Image/Advanced/TPR_FPR_Negative.gif' width='400'></center>\n",
    "\n",
    "<center><img src='Image/Advanced/Evaluation_ROC_Fit.png' width='500'></center>\n",
    "\n",
    "<center><img src='Image/Advanced/TPR_FPR_ROC.gif' width='500'>(https://angeloyeo.github.io/2020/08/05/ROC.html)</center>\n",
    "\n",
    "> **예시:**\n",
    "\n",
    "<center><img src='Image/Advanced/Evaluation_ROC_Type.png' width='900'></center>"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "393px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
